{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>   Hello and welcome to my tech blog!   My name is Lily and this is where I document all the things I do in my homelab.   I have a passion for single board computers and everything to do with Linux and Open Source Software.   I love learning about IT and I love sharing that passion.    My homelab is always growing, so this is an ideal place to follow the journey and see how I did things and what I learned in the process!   </p>"},{"location":"Backup/","title":"Backup","text":""},{"location":"Backup/#borg-backup","title":"Borg Backup","text":""},{"location":"Backup/#local-repository","title":"Local Repository","text":"<p>I use Borg Backup to backup my Laptop, PC and Pi. I backup my Laptop and PC to a remotely hosted Nextcloud instance and an external hard drive, and my Pi to a USB drive. I initialized the backups using <code>borg init --encryption repokey /path/to/repo</code> and then execute the backups via script: <pre><code>#!/bin/sh\nREPO=\"xxx\"\nSOURCE=\"xxx\"\nexport BORG_PASSPHRASE=\"xxx\"\n\nborg create --verbose $REPO::$(date +%Y-%m-%d) $SOURCE -e /home/lily/Nextcloud/\nborg prune --verbose --list $REPO --keep-daily=7 --keep-weekly=4 --keep-monthly=6\n</code></pre></p>"},{"location":"Backup/#remote-repository","title":"Remote Repository","text":"<p>On my OMV NAS, I use Borg Backup to backup to a remote repository. There's a Borg Backup Plugin you can use to configure it in the Web GUI, but I simply couldn't get it to work. You can check if the Borg connection works by executing the following command on the Borg server: <code>borg info 'ssh://server@user:&lt;sshport&gt;/./test</code>. This always worked, but OMV gave me an error message about the connection not being able to be established. The plugin basically just executes the commands to initialize a remote Borg repo, so I simply did this on the command line.</p>"},{"location":"Backup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Borg installed on both backup server and client</li> <li>SSH connection with SSH keys established from client to server</li> </ul> <p>Note</p> <p>Borg does not work with a SSH password, SSH keys must be used. Borg needs to know what keys to use, so set an environment variable for it: <code>export BORG_SSH='ssh -i /path/to/SSH_KEY</code> I would strongly recommend using a custom SSH command by changing SSH port for the server in the <code>/etc/ssh/sshd_config</code> file. A good instruction for creating SSH keys can be found here.</p>"},{"location":"Backup/#configuring-the-repo","title":"Configuring the Repo","text":"<ul> <li>Use the <code>borg info</code> command mentioned above to make sure the client can reach the backup server.</li> <li>Initialize the repo: <code>borg init --encryption repokey user@host:/path/to/repo</code> </li> </ul> <p>Note</p> <p>Make sure to copy not only the passphrase but also the repokey!</p> <ul> <li>Create a backup: <code>borg create user@host:/path/to/repo</code></li> <li>You can then use the same backup script as shown above and then create a cronjob for it with <code>crontab -e</code> so it automatically runs at regular intervalls.</li> </ul>"},{"location":"Backup/#timeshift-snapshots","title":"Timeshift Snapshots","text":"<p>I use Timeshift for system snapshots. Simply install the 'timeshift' package and use <code>man timeshift</code> to see all available options. I use rsync mode since my filesystem is ext4 - there's a btrfs mode if that's the filesystem you use - but only on systems having an Ubuntu-type subvolume layout (with @ and @home subvolumes).</p>"},{"location":"CutiePi/","title":"Raspberry Pi","text":"<p>Here you can find all the projects I did or am currently working on with my little Raspberry Pi, aka \"CutiePi\"! I am currently running most of them as Docker Containers, but this page is dedicated to the bare metal installs.</p>"},{"location":"CutiePi/#operating-system-troubleshooting","title":"Operating System &amp; Troubleshooting","text":"<p>I use Raspberry Pi OS Lite, a Debian ARM based distro and the official OS from the Raspberry Pi Foundation. I used to have DietPi OS installed, but ran into problems after the Debian Bookworm release. I couldn\u00b4t boot my Pi anymore, so I put the SD card into my laptop and mounted it to a new directory. To do this, I fist created the directory with <code>mkdir /mnt/pi</code> and then ran <code>lsblk</code> to find out the device name of the SD card. Then I mounted it to the directory with <code>sudo mnt /dev/sdX /mnt/pi</code>. I got a 'wrong fs type' error, so I used <code>mkfs -t ext4 /dev/sdX</code> to create the ext4 filesystem on it. It worked an I could now mount the card to the /mnt/pi directory and access all my files. Of course you can also just reinstall from backup, however I found this method interesting and like the flexibility that having your entire OS on an SD card offers. I then installed Raspberry Pi OS Lite on the SD card and enabled SSH on it, to copy all my config files to the Pi from my Laptop, using the 'secure copy' command:<code>scp /mnt/pi/neededfiles lily@cutiepi:/home/lily/configfiles</code>. To clean up on my laptop I used the <code>sudo umount /mnt/pi</code> and <code>sudo rm -rf /mnt/pi</code> commands.</p>"},{"location":"CutiePi/#dns","title":"DNS","text":""},{"location":"CutiePi/#pihole","title":"PiHole","text":"<p>I use PiHole as a DNS-based blocker from ads and malicious sites/tracking sites. Another useful feature is the oprion to configure local DNS A and CNAME records. I installed PiHole following the official guide. I wanted to run the web interface on a custom port, so I ran <code>sudo nvim /etc/lighttpd/lighttpd.conf</code> and searched for <code>server.port</code>. You can this value to the desired port, then access it in the browser with <code>http://&lt;hostname&gt;:&lt;port&gt;/admin</code>. I added new Adists from here, then updated Gravity (<code>pihole -g</code>). I added a script and cronjob to delete the FTL database every week (<code>crontab -e 0 0 * * 0 FTLdb.sh</code>). <pre><code>#!/bin/bash\n\nsudo systemctl stop pihole-FTL \nsudo mv /etc/pihole/pihole-FTL.db /home/lily/scripts/pihole-FTL_$(date +\"%y-%m-%d\").db #remove original DB file, create a backup file with time stamp in my scripts directory\n\ncd /home/lily/scripts\nfind . -name \"pihole-FTL_*.db\" -type f -mtime +7 -exec rm {} \\; #remove all files containing \"pihole-FTL_\" older than a week\n</code></pre></p>"},{"location":"CutiePi/#unbound-dns","title":"Unbound DNS","text":"<p>I use Unbound as a recursive DNS Server.  I configured my Pi as local DNS Server on my router, so all new clients in my network automatically use it. I followed the installation instructions from here and added it as a custom DNS Server in the PiHole DNS configuration.</p>"},{"location":"CutiePi/#dnssec","title":"DNSSEC","text":"<p>I enabled the following security options in <code>/etc/unbound/unbound.conf.d/</code>: <pre><code>server:\n    harden-below-nxdomain: yes\n    harden-referral-path: yes\n    harden-algo-downgrade: no\n    use-caps-for-id: no\n    hide-identity: yes\n    hide-version: yes\n</code></pre> To generate DNSSEC keys, run <code>sudo unbound-control-setup</code>, then restart unbound with <code>sudo systemctl restart unbound.service</code>. Using <code>dig +dnssec A www.dnssec.cz</code> you can test if DNSSEC works. The result should contain the <code>ad</code> parameter.</p>"},{"location":"CutiePi/#vpn-server","title":"VPN Server","text":"<p>I used PiVPN to turn my Pi into a VPN Server with Wireguard protocol. Now I can establish a secure connection into my home network at any time. After the inital installation, clients can be added with <code>pivpn -a</code>. I added my phone and laptop as a client. I downloaded the Wireguard App on my phone, then generated a QR code with <code>pivpn -qr</code> to establish the VPN tunnel. For my laptop, I copied the .conf file generated by PiVPN to my laptop using secure copy: <code>scp lily@cutiepi:configs/laptop.conf ~/pivpn</code>. I changed the Wireguard default port, as some networks block default VPN ports. It can be done either in the PiVPN installation wizard or later in the <code>/etc/wireguard/wg0.conf</code> <code>/etc/pivpn/setupVars.conf</code> files, but make sure to do this before generating the config files for the clients! Also make sure to enable port forwarding for the Pi on your router, so the port is accessible from outside the local network.</p>"},{"location":"CutiePi/#dyndns","title":"DynDNS","text":"<p>As my ISP changes my public IP address, I need a dynamic DNS client to make sure I can always reach my VPN Server. I registered with 'NoIP' for a DynDNS Service and then configured it on my router, so my home network has a domain name it can always be reached on, even if the IP address changes.</p>"},{"location":"CutiePi/#syncthing","title":"Syncthing","text":"<p>I use Syncthing to sync files from my Pi to my Laptop, which is especially handy via VPN. I installed Syncthing on my Pi and my Laptop. I could then access Syncthing on my Laptop browser via <code>http://localhost:8384/</code>. Then I configured a folder to sync with my Pi and tried to sync the devices with the 'Add device' button. As my Pi is headless, I thought I could simply add it via Device ID (I looked at <code>man synthing</code>, and you can display it with <code>syncthing --device-id</code>). But it didn\u00b4t work, the logs said the connection was refused by my Pi. I then set a GUI for my Pi\u00b4s Syncthing with <code>syncthing --gui-address=&lt;host-ip&gt;:8384</code>, so I could then access <code>cutiepi:8384</code> from my laptop's browser, and configured the rest from here. I needed to change the default user and password on both Syncthing instances in order to sync them.</p>"},{"location":"CutiePi/#some-notes-on-ssh","title":"Some notes on SSH","text":"<p>I would strongly recommend taking a look at SSH, as you will be using it a lot in a headless setup. I made sure to change modify the default SSH settings provided in <code>/etc/ssh/sshd_config</code>. I uncommented and changed <code>Port</code> to a custom port and <code>PasswordAuthentication</code> as well as <code>PermitRootLogin</code> to 'no'. Of course, when disabling Password Authentication, you need to make sure you have SSH keys setup with all devices you will connect to the Pi with. A good instruction for SSH key setup can be found here. I also made a SSH config file on my clients so it's less work connecting to multiple servers, as you don't need to remember all the custom ports and so on: <code>nvim ~/.ssh/config</code> and then create entries in the following format: <pre><code>Host CutiePi\n    HostName 192.168.10.xx\n    User Lily\n    IdentityFile ~/.ssh/lilyskey #enter the path to the private key\n    Port xxx #enter the custom SSH Port you set\n\nHost NAS\n    HostName 192.168.10.55\n    ...\n</code></pre> Your client will now use these parameters to connect via SSH to these hosts.</p>"},{"location":"CutiePi/#managing-disk-space","title":"Managing disk space","text":"<ul> <li>Useful commands to manage the Pi's disk space are <code>df -h</code>and <code>ncdu /</code>. The latter provides a nice overview of all files and their sizes in the specified directory (in this case, root.)</li> <li>To view a script I wrote to monitor the disk space and send notifications to my phone if it's over the threshold, see here</li> <li>Also make sure to run the apt <code>autoclean</code> and <code>autoremove</code>commands if you want to remove old packages, and package dependencies that are no longer needed.</li> <li>I also wanted to limit the space my journal log was taking up. For this, you can run <code>sudo journalctl --vacuum-time=2weeks</code> for example, to remove records older than two weeks.<ul> <li>You can also specify the maximal usage, free space, files and file size allowed for your journal by editing <code>/etc/systemd/journald.conf</code>, eg. by uncommenting and setting the line <code>SystemMaxFileSize=40M</code>. Then restart the service using <code>sudo systemctl restart systemd-journald</code> for the changes to take effect.</li> </ul> </li> </ul>"},{"location":"CutiePi/#log-rotation","title":"Log Rotation","text":"<p>You can use <code>logrotate</code> to automise log management, for example compressing logs, and deciding how long they should be kept and deleting older logs. You can do this by creating a <code>logrotate.conf</code> file (for all logs, or seperate .conf files for each log), which you fill with the options provided by <code>man logrotate</code>. For example: <pre><code>/var/log/*.log {\n# rotate log files weekly\nweekly\n# keep 4 weeks worth of backlogs\nrotate 4\n# create new (empty) log files after rotating old ones\ncreate\n# use date as a suffix of the rotated file\ndateext\n# compress log files\ncompress\n#max size\nsize 50M\n#rm rotated logs older than &lt;count&gt; days\nmaxage 30\n# packages drop log rotation information into this directory\ninclude /etc/logrotate.d\n}\n</code></pre> You can then run  <code>logrotate -v logrotate.conf</code> to execute the log rotation and check on the process, and of course create a cronjob for this.</p>"},{"location":"Docker/","title":"Docker Containers","text":"<p>After a while of installing software locally on my Raspberry Pi, I realized the benefits of using containers instead, so I started using Docker. This way, I have each application installed in a sandboxed environment for more security, all dependencies come with the container so there's no \"But it works on my machine!\" effect, and management is very convenient with easy to manage Compose or Dockerfiles that can be used with version control (Git).</p>"},{"location":"Docker/#nextcloud","title":"Nextcloud","text":"<p>I run a Nextcloud instance in a container. For this, I use the Nextcloud container that contains an Apache image to run the webserver, found here. I installed using the provided Docker Compose file, but changed the volume mappings to <code>volumes : - ./cloud:/var/www/html - ./data:/var/www/html/data</code> so I didn't have to navigate to <code>/var/lib/docker/volumes/nextcloud</code> to find the Nextcloud files on the Docker host. Anything I put in the data volume on the host will appear on <code>hostip:8080</code> on my webbrowser, and the other way around. Note: You need to make sure the correct permissions are set (<code>sudo chown -R www-data:www-data /home/lily/nextcloud &amp;&amp; sudo chmod -R 755 /home/lily/nextcloud</code>) so the Nextcloud user ('www-data') has access to the data. I also had some problems with data being in the Nextcloud cache but not written on the disk after deletion of a user, which prevented file sync. I resolved this by running some troubleshooting commands on the Docker host inside the Nextcloud container: <pre><code>docker exec -u www-data nextcloud php occ files:cleanup #clear file cache in the Docker container\ndocker exec -u www-data nextcloud php occ files:scan --all #rescan all files\ndocker exec -u www-data nextcloud php occ maintenance:repair #maintenance repairs, eg. to fix database inconsistencies, adjust file paths, and address other issues that may arise in a Nextcloud installation\n</code></pre> If you want access Nextcloud through your domain's URL, you will need to edit the <code>config.php</code> file to include <code>'*.yourdomain.com'</code> in the trusted domains section.</p>"},{"location":"Docker/#uptime-kuma","title":"Uptime Kuma","text":"<p>I use Uptime Kuma, a simple yet powerful monitoring tool, to receive notifications about my services. You can configure lots of notification options, like Mail (simply google your mail provider + SMTP server), Signal, Pushover, etc. I gave Uptime Kuma read only rights to the Docker socket, so I can configure the Pi as a Docker Host in Uptime Kuma, and comfortably monitor all my containers with it.  Another cool feature is the 'Upside Down' Status monitor, which I explain here But as I want to prevent Docker containers not running in the first place, I wrote this script to restart them if they go down.</p>"},{"location":"Docker/#traefik","title":"Traefik","text":""},{"location":"Docker/#docker-composeyml-and-traefikyml","title":"docker-compose.yml and traefik.yml","text":"<p>I use Traefik as a reverse proxy and loadbalancer. I also configured self-signed SSL certificates with it, using the ACME protocol. I did this by registering the 'witchessabath' Domain with Cloudflare, and getting a Cloudflare DNS API Token for TLS encryption. To create an API token, I logged in to Cloudflare's 'My Profile' page, then navigate to <code>API token</code> in the menu and created a token using a custom template:  Above: The seetings needed for the API Key: Zone &gt; Zone &gt; Read and Zone &gt; DNS &gt; Write  In my Traefik Docker Compose file I then added the configuration: <pre><code> volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - ./conf/traefik.yml:/traefik.yml:ro\n      -  ./certs:/etc/traefik/certs:rw\n    environment:\n      - CF_DNS_API_TOKEN=xxx\n</code></pre> Traefik will now use <code>traefik.yml</code> as a configuration file and write the certificate data into the <code>acme.json</code> file in the /certs directory.</p>"},{"location":"Docker/#labels","title":"Labels","text":"<p>To use my self-signed TLS certificates for my Docker containers, I give them the following labels: <pre><code>- traefik.enable=true\n- traefik.http.routers.paperless.entrypoints=web,websecure #configure for HTTP or HTTPS traffic/HTTPS redirection\n- traefik.http.routers.paperless.rule=Host(`app.witchessabath.com`)\n- traefik.http.routers.paperless.tls=true\n- traefik.http.routers.paperless.tls.certresolver=cloudflare #enter the name of the certificate resolver configured in traefik.yml\n</code></pre></p> <p>Note</p> <p>Be careful about the backticks: I originally used single quotes instead of backticks for the 'Host' label, and ran into an error (called 'invalid rune' in Go)</p> <p>If a container already exists, you can also edit its labels with Portainer. Note that the containers must be in the same Docker network as the Traefik container. For non-Docker containers or services you didn't attach labels to, simply add the service to the traefik Docker Compose file.</p>"},{"location":"Docker/#watchtower","title":"Watchtower","text":"<p>I use Watchtower to keep my Docker containers up to date. Watchtower searches local or online repositories for newer versions of the installed containers, and then updates them with the exact same settings that were configured before. Watchtower is itself a container installed with  <pre><code>docker run -d \\\n  --name watchtower \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  containrrr/watchtower\n</code></pre></p>"},{"location":"Docker/#homeassistant","title":"Homeassistant","text":"<p>I installed Homeassistant with the Docker Compose file provided on their website. Make sure to map a config directory used on your host for the config file. I currently only use it for the Wake On LAN integration in my home network. To set this up, do the following:</p> <ul> <li>check if Wake On LAN is enabled on the target device by running <code>ethtools &lt;ethernet_interface&gt; | grep \"Wake-on\"</code>. Wake-On should return letter g for activated</li> <li>To permanently enable WOL, add the line <code>post-up /usr/sbin/ethtool -s &lt;ethernet_interface&gt; wol g</code> to <code>/etc/network/interfaces</code> on the target computer</li> <li>install the \"wakeonlan\" packet on your server and run a test with the <code>wakeonlan &lt;target_macaddress&gt;</code> command     it should return <code>Sending magic packet to 255.255.255.255:9 with &lt;target_macaddress&gt;</code></li> <li>add the following to your Homeassistant configuration.yaml: <pre><code>wake_on_lan:\n\nswitch:\n  - platform: wake_on_lan\n    mac: &lt;target_macaddress&gt;\n    name: explorer\n\n  - platform: wake_on_lan\n    mac: &lt;target_macaddress_2&gt;\n    name: juno\n</code></pre> This will create two new entities called \"explorer\" and \"juno\" with the specified MAC addresses and the Wake On Lan integration. The entities can be found in the Web UI (host:8123) under Settings &gt; Devices &amp; Services &gt; Entities.  You can now go to your dashboard and select <code>Add Card &gt; Entity</code> to add the WoL switches to your dashboard.</li> </ul> <p>Note</p> <p>When naming the config file, make sure it's called configuration.yaml. I created it as a .yml file first and the configuration could not be applied. I found this out by using the very handy <code>docker exec &lt;home_assistant_container_name&gt; hass --script check_config</code> command which troubleshoots the configuration for you.</p>"},{"location":"Docker/#homarr","title":"Homarr","text":"<p> Above: Screenshot of my Homarr Dashboard</p> <p>I use Homarr to have a dashboard overview of the services I need to access on my Pi. It also has cool API integrations, for example to display PiHole data. I installed it using Docker Compose, then configured it via the GUI webinterface. As I wanted to use a custom icon for the tab bar and on the site itself, I downloaded <code>dino.png</code> on my laptop and moved it to <code>~/homarr/icons</code> on my Pi, which I mapped to the logo directory in homarr using the line <code>- ./icons:/app/public/imgs/logo</code> under the <code>volumes:</code> section of my Docker Compose file. You could also connect to the container CLI, navigate to the logo directory and use <code>wget &lt;icon URL&gt;</code> to download the icon in the container directly. You can then change the icon by specifying the path to it (in the container) in the 'Settings' page on the Web GUI.</p>"},{"location":"Docker/#portainer","title":"Portainer","text":"<p>I use Portainer to have an overview and comfortable GUI management interface for all my Docker containers. I find creating and managing Docker networks, assigning labels to containers, as well as reviewing container logs more comfortable through Portainer. It also enables you to connect to a console and act as root user in the container, therefore replacing the need to use the <code>docker exec -it &lt;containername&gt; sh</code> command to access the container's CLI. Portainer can be installed by creating a volume (<code>docker volume create portainer_data</code>) and then pulling the container for it (<code>docker run -d -p 8000:8000 -p 9443:9443 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:latest</code>).</p>"},{"location":"Docker/#paperless","title":"Paperless","text":"<p>I use Paperless as a document managagement system. I configured it using a Docker Compose file, and added an environment file for more fine-grained configuraions with the entry <code>env_file: docker-compose.env</code>. The database I use with it is Redis. (This is one of the benefits of using Docker Compose - simply start the file with <code>services:</code> and define multiple services in the file.)</p>"},{"location":"Docker/#troubleshooting","title":"Troubleshooting","text":"<p>Some useful troubleshooting commands:</p> <ul> <li><code>docker logs &lt;container_name&gt;</code> to view container logs</li> <li><code>docker compose up -d --force-recreate</code> to activate changes after editing the compose file of a running Docker Compose container</li> <li><code>docker exec -it &lt;container_name&gt; /bin/sh</code> to connect to the shell of the container</li> <li><code>docker ps</code> and <code>docker network ls</code> to list running containers and Docker networks</li> <li><code>docker inspect &lt;container_name&gt;</code> and <code>docker network inspect &lt;network_name&gt;</code> for details about the container/network</li> </ul>"},{"location":"NAS/","title":"File Sharing","text":""},{"location":"NAS/#network-attached-storage","title":"Network Attached Storage","text":"<p>I used Openmediavualt (OMV) to make a NAS out of my Raspberry Pi 3B+, to which I simply attached a HDD.</p> <p> Above: Screenshot of my OMV Dashboard</p>"},{"location":"NAS/#preparations","title":"Preparations","text":"<ul> <li>Install Raspberry Pi OS Lite</li> <li>Install OMV using the installation script.</li> <li>Wipe the drive(s) you wish to connect to your Pi. I did it like this:<ul> <li>Connect drive to PC, find out the device name by using <code>sudo fdisk -l</code> or <code>lsblk</code></li> <li>Use dd command to write zeros to the drive: <code>sudo dd if=/dev/zero of=/dev/sbdX bs=5M status=progress</code></li> </ul> </li> <li>Log onto the OMV Web GUI with the default <code>admin</code> user and <code>openmediavault</code> password, then change them and setup a new user Make sure to put that user in the <code>SSH</code> group in the <code>Users</code> Tab, to ensure SSH connectivity</li> <li>I installed the <code>openmediavault-sharerootfs</code> plugin, to be able to share folders located on the root filesystem</li> </ul> <p>Note</p> <p>You will still only be able to manage your disks, shares etc. with the admin user</p> <ul> <li>Check if disk is found under <code>Storage &gt; Disks</code></li> <li>Create and mount a file system on that disk under <code>Storage &gt; File Systems</code> - I used ext4</li> </ul>"},{"location":"NAS/#smb-share-setup","title":"SMB Share Setup","text":"<ul> <li>Create a folder on the disk under <code>Storage - Shared Folders</code></li> <li>Make the SMB share browseable and configure other settings like minimum SMB version and permission inheritance under <code>Services &gt; SMB/CIFS &gt; Settings</code></li> <li>Create the SMB Share in <code>Services &gt; SMB/CIFS &gt; Shares</code></li> <li>Then add the share as a network drive in Windows, using the user credentials created before</li> </ul>"},{"location":"NAS/#iscsi-lun-setup","title":"iSCSI LUN Setup","text":"<ul> <li>Install the <code>openmediavault-tgt</code> plugin</li> <li>Go to <code>Services &gt; tgt &gt; Settings &gt; Check \"Enable\"</code></li> </ul>"},{"location":"NAS/#create-an-iscsi-image","title":"Create an iSCSI image","text":"<p>There's two ways of doing this, by terminal or in the Web GUI.</p> TerminalGUI <p>Connect to the terminal locally or via SSH, and make a directory for the image, eg. <code>sudo mkdir /iscsiimg</code>, then use dd to make the empty image: <code>sudo dd if=/dev/zero of=/iscsiimg/lun1.img bs=1M count=5120</code></p> <p>Note</p> <p>Don't forget the <code>count</code> parameter, to only make an image of your desired size (it counts blocks of the size specified in the <code>bs</code> parameter). Otherwise <code>dd</code> will copy an infinite stream of zeroes and wipe everything else out.</p> <p>Go to <code>Services &gt; tgt &gt; Images</code> and click \"Create\", then enter the full path you want the image to have, eg. \"/iscisimg.lun1.img\".</p>"},{"location":"NAS/#create-an-iscsi-target","title":"Create an iSCSI Target","text":"<p>Go to <code>Services &gt; tgt &gt; Targets</code> and configure a target, with the image you just created as 'Backing Store'. As initiator I used my laptop and entered its hostname.</p> <p>Note</p> <p>You also need to check the \"Enabled\" box for each target, otherwise it won't be found by the initiator.</p>"},{"location":"NAS/#on-the-initiator","title":"On the initiator:","text":"<ul> <li>Install package <code>open-iscsi</code></li> <li>Start iSCSI daemon with <code>sudo systemctl start iscsid</code>, if you want it to start on system startup automatically use <code>sudo systemctl enable iscsid</code></li> <li>Discover the iSCSI target: run <code>sudo iscsiadm -m discovery -t st -p &lt;OMV IP Adress&gt;</code></li> <li>Login using command <code>sudo iscsiadm -m node -T &lt;ISCSI Target IQN&gt; -p &lt;OMV IP Adress&gt; --login</code>. You can find the target's IQN in the OMV web GUI in the <code>tgt &gt; Targets</code> page next to the name of the target you just created.</li> <li>Use the command <code>lsblk</code> to see the device name the LUN has on your system.</li> <li>Create a directory and then mount the LUN to it (<code>sudo mkdir /mnt/iscsi</code>, then <code>sudo mnt /dev/sdX /mnt/iscsi</code>)</li> <li>I got an error saying the image has the 'wrong fs type', as it has no filesystem configured, so I simply used <code>sudo mkfs -t ext4 /dev/sdX</code> to give it an ext4 filesystem</li> <li>You can now put files here. You can logout with <code>sudo iscsiadm -m node -T &lt;ISCSI Target IQN&gt; -p &lt;OMV IP Adress&gt; --logout</code>, and also just unmount and mount the LUN if you wish.</li> </ul>"},{"location":"NAS/#samba","title":"Samba","text":"<p>I also created a Samba Share on my Raspberry Pi for my home network. To do this, install the packages <code>samba samba-common smbclient</code> and make a backup of the <code>/etc/samba/smb.conf</code> file, then create a new one with your custom entries. Mine simply looks like this: <pre><code>[global]\nworkgroup = WORKGROUP\nsecurity = user\nclient min protocol = SMB2\nclient max protocol = SMB3\nmap to guest = Bad User\nguest account = nobody\n\n[Samba]\npath = /mnt/sambavolume\nread only = no\nguest ok = yes\n</code></pre> This gives me the Share 'Samba'. Guest accounts are allowed, so no login is needed. The standard guest account is the Unix user 'nobody'. To ensure that everyone has permissions to the share, you also need to set them for the directory: <code>sudo chmod 777 /mnt/sambavolume</code>. If you want to only enable access for certain users, don't enable guest access in the configuration file and instead set the <code>valid users =</code> option for the samba share. Then use <code>sudo smbpasswd -a &lt;user&gt;</code> to set login credentials to the share for the user.</p>"},{"location":"NAS/#creating-a-logical-volume-for-the-samba-file-share","title":"Creating a logical volume for the Samba file share","text":"<p>I wanted to set a size restriction for the Samba file share, so I decided to map it to a logical volume. You can map the Samba file share to any directory you want using the <code>path =</code> option. To map it to a logical volume, I connected my Pi to an external SDD and did the follwing:</p> <ul> <li>install <code>lvm2</code> package</li> <li>use the <code>lsblk</code> command to find out the name of the SDD block device and create a physical volume on it using <code>sudo pvcreate /dev/sda</code></li> <li>create a volume group on the phyiscal volume: <code>vgcreate volgroup1 /dev/sda</code></li> <li>create the logical volume: <code>sudo lvcreate -L 250G volgroup1 -n sambavolume</code>. Now I have a 250 GiB volume called 'sambavolume'</li> <li>create ext4 filesystem on the volume: <code>mkfs.ext4 /dev/mapper/volgroup1-sambavolume</code> (as a logical volume is not a regular block device, all logical volumes are found under <code>/dev/mapper</code>. <code>/dev/mapper/volgroup1-sambavolume</code> is the full path to my sambavolume.)</li> <li><code>sudo mkdir -p /mnt/sambavolume</code> and <code>sudo mount /dev/mapper/volgroup1-sambavolume /mnt/sambavolume</code> to mount the logical volume.</li> <li>to make persistent after reboots, I added an entry for the volume to <code>/etc/fstab</code>: <code>/dev/mapper/volgroup1-sambavolume /mnt/sambavolume ext4 defaults 0 2</code></li> </ul>"},{"location":"NAS/#connecting-to-the-share-from-windows-devices","title":"Connecting to the share from Windows devices","text":"<p>To connect to the share using Windows devices, simply enter <code>\\\\&lt;serverIP&gt;\\&lt;sambasharename&gt;</code> in the file explorer or map as new network device. So in my case, <code>\\\\cutiepi\\samba</code>. You are now automatically connected to whatever directory is specified as 'path' in the smb.conf file for this share.</p>"},{"location":"miscellaneous/","title":"Miscellaneous","text":"<p>Here I will share some miscellaneous projects that don't really have anything to do with the rest of my homelab.</p>"},{"location":"miscellaneous/#arduino-weather-station","title":"Arduino Weather Station","text":"<p>I made a little 'Weather Station' to display the current weather in my city using an Arduino, a LCD display and the OpenWeatherMap API. I did the following steps to make it work:</p> <ul> <li>Install the Arduino IDE on my PC</li> <li>Connect the Arduino to my PC and select the correct COM port, so the code can be loaded from the IDE to the Arduino</li> <li>Connect the Arduino to the LCD display with jumper cables</li> <li>Retrieve an API Key from OpenWeatherMap</li> <li>Write the code, which you can find here.</li> </ul> <p>Note</p> <p>Make sure to enter the correct data types for the JSON values (e.g. double for Temperature) - the Arduino_JSON.h library doesn't allow the <code>Convert.toDouble</code> method. Also, note that OpenWeatherMap display Temperature in Kelvin - hence the line <code>double temperatureC = (temperatureK - 273.15);</code> to convert it to Celsius. Another tip: In the Arduino IDE, go to 'Tools -&gt; Serial Monitor' to view the <code>Serial.println</code> lines. Make sure you have the correct Baud rate selected.</p>"},{"location":"miscellaneous/#arch-linux-configuration","title":"Arch Linux Configuration","text":"<p>A quick section to explain some things I had to setup myself when using Arch:</p>"},{"location":"miscellaneous/#how-to-display-appimages-in-rofi","title":"How to display AppImages in Rofi","text":"<p>I use rofi as an app launcher. It works great, but by default only displays and starts apps installed by the package manager. After installing AppImages, I wanted them to be accessible from rofi as well. To enable this, I did the following: - <code>chmod +x Example.AppImage</code> to make the AppImage executable - <code>sudo nvim /usr/share/applications/Example.desktop</code> to create a desktop file for the AppImage     Content:     <pre><code>[Desktop Entry]\nType=Appication\nName=Example\nExec=/path/to/Example.AppImage\n</code></pre> Now rofi can also launch the AppImage.</p>"},{"location":"miscellaneous/#scripting-notfications-for-battery-usage","title":"Scripting notfications for battery usage","text":"<p>I noticed when writing a script to notify me when battery is low, that notifications can't be delivered. The script is: <pre><code>#!/bin/bash\nbat_files=\"/sys/class/power_supply/BAT1\"\nbat_status=$(cat \"${bat_files}/status\")\ncapacity=$(cat \"${bat_files}/capacity\")\necho \"${capacity}\"\nif [[ \"${bat_status}\"==\"Discharging\" &amp;&amp; ${capacity} -le 25 ]]; then\n    echo \"Battery alert - ${capacity}%\"\n    notify-send \\\n        \"Warning: Battery! Only ${capacity}% battery remaining!\"\nfi \nif [[ \"${bat_status}\"==\"Discharging\" &amp;&amp; ${capacity} -le 10 ]]; then\n    echo \"Battery alert - ${capacity}%\"\n    notify-send \\\n        \"Urgent Warning! Only ${capacity}% battery remaining! Feed me!\"\nfi\n</code></pre> Everything was working but the <code>notify-send</code> command. I tried making the notification daemon a D-Bus service by configuring a <code>org.freedesktop.Notifications.service</code> file in the D-Bus service directory, but it still didn't reliably start. I then realized I hadn't enabled dunst via <code>sudo systemctl enable dunst.service</code>. Only using the <code>systemctl start</code> command will start the service in that session, but to be able to automatically a service, the <code>enable</code> command is needed. Then, I could simply autostart dunst via my window manager i3 (<code>exec-always --no-startup-id dunst</code>).</p>"},{"location":"miscellaneous/#video-game","title":"Video Game","text":"<p>In 2023, I used Game Maker Studio to make a 2D RPG computer game as a birthday gift. The engine uses its own language called Game Maker Studio, and I can highly recommend it, as I had a lot of fun and a great learning experience. I chose this engine because  it was used by Toby Fox to make 'Undertale', a great game that majorly inspired my project. I can highly recommend Peyton Burnham's YouTube videos on the engine. I also uploaded a tiny bit of my code into this GitHub repo, so you can have a sneak peek at what the Game Maker Script Language looks like if you wish.</p>"},{"location":"proxmox/","title":"Proxmox VE","text":"<p>Here are some notes on my Proxmox instance. Proxmox is a great type I hypervisor. I mainly use it to test software I don't want to immediately install in my production homelab environment.</p>"},{"location":"proxmox/#installation-and-initial-configuration","title":"Installation and initial configuration","text":"<p>Here are the steps I took for a basic Proxmox install and setup to run some LXC containers:</p> <ul> <li>Download the official Proxmox VE ISO image to a bootable USB drive</li> <li>Go to the BIOS of your PC and enable virtualisation support<ul> <li>As I am running Proxmox on a HP PC with an Intel CPU, the virtualisation option to be enbled was 'System Options -&gt; Virtualization Technology (VTx)'</li> </ul> </li> <li>Go through the initial Proxmox install instructions for Proxmox VE with GUI</li> <li>Now Proxmox is running and can be configured on the web interface <code>https://(hostIP):8006</code></li> <li>Name your cluster. The host 'pve' is now a node in this cluster.</li> <li>Select the node in the side menu, then navigate to 'Updates &gt; Repositories'<ul> <li>Here you can select the software repositories your system can install packages from. There are some default repositories, here I disabled the Proxmox Enterprise repository as I have no enterprise subscription. Instead I clicked on 'Add' and added the <code>pve-no-subscription</code> repo.</li> <li>Now you can go back to 'Update' and click 'Refresh', then 'Upgrade'. This performs an <code>apt update &amp;&amp; upgrade</code> on the node.</li> <li>If you use Proxmox without a subscription, a window pops up to remind you it's not recommend. This can be disabled by changing the <code>proxmoxlib.js</code> file and then restarting the service for the webinterface (<code>pveproxy.service</code>), like in these instructions.</li> </ul> </li> </ul>"},{"location":"proxmox/#lxc-container-setup","title":"LXC Container setup","text":"<p>You can run either VMs or LXC Containers (Linux Containers) on Proxmox VE. I chose to first deploy some containers, as I wanted to run Linux systems and containers share the same kernel with the host. This makes them really lightweight and fast. To deploy a container, go to the node in your sidemenu and select the menupoint <code>local</code>. Here you will find a sub-menu with the point <code>Container Templates</code>, where you can browse a big list of Linux images, which you can download. These templates can be used for deploying containers with this OS. There's also preconfigured containers that have certain software already installed. After the template has finished downloading, you can click the big <code>Create CT</code> button on the top right of the Web GUI. This will start a setup wizard where you can choose the node it will run on, create a user/password and SSH keys, and the virtual hardware specs.</p> <p>Note</p> <p>I chose 'Use Host DNS Settings' in the container setup wizard, but as I wanted to install software, I noticed DNS was not working for me. I had to manually edit the container configuration file on the pve node (<code>/etc/pve/lxc/&lt;container_id&gt;.conf</code>). I added the line <code>lxc.environment = DNS_SERVER=1.1.1.1 1.0.0.1</code>, then entered the container from the command line with <code>pct enter &lt;container_id&gt;</code> and adjusted its <code>/etc/resolv.conf</code> file to use these nameservers as well. Then DNS resolution worked and I could run my first <code>apt update &amp;&amp; upgrade</code> and then install packages.</p>"},{"location":"proxmox/#vm-setup","title":"VM Setup","text":"<p>In the side menu under your node, select <code>local</code> then go to <code>Content &gt; ISO Images</code> and click <code>Upload</code>. Now you can upload an ISO from your local PC. Then go to <code>Create VM</code> on the top right. I did this as I wanted a graphical Debian install.</p>"},{"location":"proxmox/#nfs-server","title":"NFS Server","text":"<p>I used my VM to setup an NFS Server by doing the following:</p> <ul> <li>install the <code>nfs-kernel-server</code> package</li> <li>create a directory for the share, eg. <code>sudo mkdir -p /srv/nfs/share</code> and set the permissions<ul> <li>As I'm not doing this in a production evironment, I used <code>chown nobody:nogroup</code> and <code>chmod 755</code></li> </ul> </li> <li>edit the <code>/etc/exports</code> file to specify allowed clients. I wanted to allow all clients in my network, so I added the line <code>/srv/nfs/share *(rw,sync,no_subtree_check)</code></li> <li>then update the NFS Server configuration by executing <code>exportfs -ra</code></li> </ul>"},{"location":"proxmox/#firewall","title":"Firewall","text":"<p>In the Datacenter menu, expand the <code>Firewall</code> menu point.  I recommend first going to <code>Security Groups</code> and creating a new group - which is simply a set of rules.  These can then later be added to both the node(s) and the VMs/Containers running on the system. This is very practical, as you don't have to create some standard rules over and over again to apply to different nodes or VMs. In the Security Group, you can click the <code>Add</code> button to create rules. A window will pop up with the option to create rules applying either a predefined Macro (e.g. HTTPS or FTP), or by selecting the protocol.  You can add seperate rules on every layer of the system, from the whole datacenter to a single container. They will be applied from the first rule on the <code>Firewall</code> menu down to the last rule, so be careful about your rule order.</p> <p>Note</p> <p>An important aspect in Proxmox is that you should create these rules first and then enable the firewall - because it's disabled by default. You can do so in the <code>Firewall &gt; Options</code> menu. Be careful if you enable the firewall before creating any rules - because the default policy is set to <code>DROP</code> (but can be changed here). So you might accidentally lock yourself out of your system, if you don't create some <code>ACCEPT</code> rules first. </p> <p> Above: The Firewall Options menu, where the firewall must first be enabled on the Datacenter level  I created a Security Group for my VPN connection to allow HTTP, HTTPS, and TCP traffic on port 8006 to allow access to Proxmox from my VPN. (This is very practical as I also enabled Wake on LAN fo the Proxmox node, and now I can access my containers from anywhere when I need them, without having to keep the PC running 24/7.)</p> <p>Note</p> <p>Be sure to create the rule for port 8006/the port where the Proxmox VE API is exposed: otherwise you won't be able to access the Proxmox web interface, even with HTTP(S) enabled. </p> <p> Above: Screenshot of a VPN security group, made by selecting Macros (HTTP/HTTPS) or a protocol and destination port (TCP to port 8006). </p>"},{"location":"proxmox/#tls-certificates","title":"TLS Certificates","text":"<p>To deploy self signed TLS certificates:</p> <ul> <li>navigate to <code>Datacenter &gt; ACME</code>.</li> <li>click <code>Challenge Plugins &gt; Add</code> to add a DNS challenge from your provider. I use Cloudflare, so I entered my Cloudflare mail and created a Cloudflare API token.</li> <li>To create an API token, I logged in to Cloudflare's 'My Profile' page, then navigate to <code>API token</code> in the menu and created a token using a custom template:  Above: The seetings needed for the API Key: Zone &gt; Zone &gt; Read and Zone &gt; DNS &gt; Write </li> <li>go to <code>Accounts &gt; Add</code> and add the mail address used for the DNS Challenge Plugin. Now, select 'Let's Encrypt V2' from the Dropdown Menu</li> <li>next, go to your node(s) and select <code>Certificates</code> from the side menu. Under <code>ACME</code>, click <code>Add</code> and add a domain and select 'DNS' under <code>Plugin Type</code> </li> <li>In the top menu, select the previously created account in the <code>Using account:</code> dropdown menu, then click <code>Apply</code> Above: ACME configuration for my PVE node </li> </ul>"},{"location":"proxmox/#backups","title":"Backups","text":"<p>Here's how I setup a backup to my NFS Share for the VM file systems:</p> <ul> <li>On your Proxmox Datacenter tab, go to <code>Storage</code> and click <code>Add &gt; NFS</code></li> <li>Enter a name in the <code>ID</code> field, then enter the server IP and select the NFS Share path under <code>Export</code>. For content, make sure to select the option <code>VZDump backup files</code></li> <li>Then go to <code>Backup</code>, and click <code>Add</code> to add a backup job. Select your NFS Share as storage and determine a schedule for the backup to run.  Make sure to keep more than the last backup by modifying the retention time in the <code>Retention</code> tab.</li> </ul>"},{"location":"proxmox/#users-and-2fa-authentication","title":"Users and 2FA Authentication","text":"<p>In the Datacenter Menu, go to <code>Permissions &gt; Users</code> to create a new user. In the <code>Two Factor</code> menu point beneath that, you can set up 2FA, for example with an authenticator app.</p>"},{"location":"proxmox/#hostname-change","title":"Hostname Change","text":"<p>I changed the hostname using the <code>hostnamectl set-hostname</code> command, but I did not realize that in Proxmox, the hostname is also the node name. That's why after a reboot, the naming was no longer consistent and I could no longer access the web interface. Instead, you need to edit both <code>/etc/hosts</code> and <code>/etc/hostname</code> and replace all instances of the old hostname. If your node is in a cluster, make sure to also change the node name in <code>/etc/pve/corosync.conf</code>.</p>"},{"location":"scripts/","title":"Automation","text":"<p>Some simple scripts and Ansible files I wrote for automation purposes.</p>"},{"location":"scripts/#notifications","title":"Notifications","text":""},{"location":"scripts/#uptime-kuma","title":"Uptime Kuma","text":"<p>Currently I use Uptime Kuma as a monitoring tool.  Some things I cannot monitor with Uptime Kuma, so I wrote scripts and included a push notification to Uptime Kuma in them, so I know when a certain part of a script failed. To do this, go to <code>Uptime Kuma &gt; Add New Monitor &gt; Monitor Type \"Push\"</code>. Scroll down and check the Box next to <code>Upside Down Mode</code>. The new monitor displays the push URL at the top, simply copy it and add it to your script with <code>curl</code>. Now, when the script calls the Push URL, you will now it failed. </p> <p>Note</p> <p>Make sure the client knows the hostname of the service Uptime Kuma is running on as it's included in the URL, or simply replace with localhost if the script is running on the same server as Kuma.</p>"},{"location":"scripts/#healthchecksio","title":"Healthchecks.io","text":"<p>You can also use Healthcheck.io to check if the scripts themselves ran by appending <code>curl --retry 3 https://hc-ping.com/your-uuid-here/$?</code> to them, or to monitor cronjobs. Healthchecks can send you success or failure messages through your chosen integration, and it can even measure job execution time.</p>"},{"location":"scripts/#scripts","title":"Scripts","text":""},{"location":"scripts/#monitoring-disk-space","title":"Monitoring disk space","text":"<p>This script will monitor the disk space on my servers, and send a notification to my phone if a certain threshold is reached, using Uptime Kuma.  I created a monitored cronjob so it will run twice a week (<code>crontab -e 0 20 * * 2,5 /home/lily/scripts/diskmonitor.sh &amp;&amp; curl -fsS -m 10 --retry 5 -o /dev/null https://hc-ping.com/my-uuid</code>) <pre><code>#!/bin/bash \n\n#define a threshold in megabytes                              \nTHRESHOLD=600                                                                                                                                                                              #define a directory to be monitored (in this case root)                                                   \ntarget=/                                                                                                                                                                                   \n#check disk usage in machine readable format, excluding /proc to not include running processes, storing only the size in the variable\nusage=$(sudo du -sm --exclude=\"/proc\" \"$target\" | awk '{print$1}')\n#calculate remaining space until threshold is reached \nremaining=$((THRESHOLD - usage))\nif [ \"$usage\" -gt \"$THRESHOLD\" ]; then\n    echo \"Warning, Threshold exceeded! Disk is at \"$usage\"M\"\n    #send push message to Uptima Kuma\n    curl http://localhost:3025/api/push/EfH8ApJpQu?status=up&amp;msg=OK&amp;ping=\nelse\necho \"There are \"$remaining\"M remaining\"  \nfi\n</code></pre></p>"},{"location":"scripts/#keep-docker-compose-containers-running","title":"Keep Docker Compose containers running","text":"<p>This script will ssh into remote Docker host, check if any containers have exited, and if so start them up again. Note: This works quite easily as I name my Docker Compose directories exactly like the containers. I run this once a day (<code>crontab -e 0 12 * * * /home/lily/scripts/dockermonitor.sh</code>), you could also have it running in the background using <code>nohup ./dockermonitor.sh &amp;</code>. <pre><code>#!/bin/bash\n\nREMOTE_HOST=\"lily@cutiepi.local\"\nKEYFILE=\"/home/lily/.ssh/cutiepi\"\n\n#SSH into the server and store exited containers in variable\nssh -i $KEYFILE -o StrictHostKeyChecking=no $REMOTE_HOST &lt;&lt; EOF \nexited_containers=$(docker ps --filter status=exited --format \"{{.Names}}\") \n\n#if variable not empty &gt; cd into the container directories and run the docker compose up command\nif [[ -n \"\\$exited_containers\" ]], then\n    for container_name in \\$exited_containers; do\n        echo \"Container \\$container_name has exited\"\n        cd \\$container_name || { \n            echo \"Directory \\$container_name not found\"\n            continue\n        }\n\n        docker compose up -d\n\n        running_container=$(docker ps --filter \"name=\\$container_name\" --filter \"status=running\" --format \"{{.Names}}\")\n        if [[ -n \"\\$running_container\" ]]; then\n            echo \"Container \\$container_name is running again\"\n         else\n            #if containers can't be restarted, send push notification to Uptime Kuma\n            echo \"Failed to start container \\$container_name\"\n            curl http://localhost:3025/api/push/bCe0PPb4OZ?status=up&amp;msg=OK&amp;ping=\n        fi\n\n         cd\n    done\nelse\n    echo \"No exited containers\"\nfi\nEOF\n</code></pre></p>"},{"location":"scripts/#monitoring-processes","title":"Monitoring processes","text":"<p>Simple script to quickly check for running processes. <pre><code>#!/bin/bash\necho \"Enter process name: \"   \nread process\n\nif pgrep \"$process\" &gt; /dev/null 2&gt;&amp;1; then\n    echo \"Process is running\"\nelse\n    echo \"Process is not running.\"\nfi\n</code></pre></p>"},{"location":"scripts/#ansible","title":"Ansible","text":"<p>I have a short Ansible playbook for updating my Linux hosts. Prerequisites are:</p> <ul> <li>installation of the <code>ansible-core</code> package</li> <li>root ssh keys to enable root login to remote machines via ssh </li> </ul> <p>I created the following <code>tasks.yaml</code> file for the updates: <pre><code>---\n- name: Update servers\n  hosts: linuxhosts\n  remote_user: root\n\n  tasks:\n  - name: apt update\n    ansible.builtin.apt:\n      update_cache: true\n\n  - name: apt upgrade\n    ansible.builtin.apt:\n      upgrade: \"safe\"\n\n  - name: apt autoremove\n    ansible.builtin.apt:\n      autoremove: true\n</code></pre> You can run this playbook using <code>ansible-playbook -i hosts.ini tasks.yaml</code>. Make sure you have a hostgroup in the hosts.ini file called 'linuxhosts'. I automated the updates with the following script: <pre><code>#!/bin/bash\n\n# cd to the directory containing the playbook and hosts file\ncd /home/lily/ansible/update/ || exit 1\n# execute the playbook\nansible-playbook -i hosts.ini tasks.yaml\n\n# check if last command was successful. If not, send a push notification to Uptime Kuma\nif [[ $? -eq 0 ]]; then\n  echo \"Linux Hosts were updated automatically\"\nelse\n  curl \"http://cutiepi:3025/api/push/stAo7759Ml\"\nfi\ncd ~\n</code></pre></p>"}]}